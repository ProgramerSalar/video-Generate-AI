{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.6.3.tar.gz (2.6 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flash-attn) (2.4.1+cu118)\n",
      "Collecting einops (from flash-attn)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->flash-attn) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->flash-attn) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->flash-attn) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->flash-attn) (70.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\manjusha kumari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py): started\n",
      "  Building wheel for flash-attn (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for flash-attn\n",
      "Failed to build flash-attn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [120 lines of output]\n",
      "      fatal: not a git repository (or any of the parent directories): .git\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.4.1+cu118\n",
      "      \n",
      "      \n",
      "      C:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Requirements should be satisfied by a PEP 517 installer.\n",
      "              If you are using pip, you can try `pip install --use-pep517`.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        dist.fetch_build_eggs(dist.setup_requires)\n",
      "      running bdist_wheel\n",
      "      Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu118torch2.4cxx11abiFALSE-cp312-cp312-win_amd64.whl\n",
      "      Precompiled wheel not found. Building from source...\n",
      "      C:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:495: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "        warnings.warn(msg.format('we could not find ninja.'))\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-312\n",
      "      creating build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      copying flash_attn\\bert_padding.py -> build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      copying flash_attn\\flash_attn_interface.py -> build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      copying flash_attn\\flash_attn_triton.py -> build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      copying flash_attn\\flash_attn_triton_og.py -> build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      copying flash_attn\\flash_blocksparse_attention.py -> build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      copying flash_attn\\flash_blocksparse_attn_interface.py -> build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      copying flash_attn\\fused_softmax.py -> build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      copying flash_attn\\__init__.py -> build\\lib.win-amd64-cpython-312\\flash_attn\n",
      "      creating build\\lib.win-amd64-cpython-312\\hopper\n",
      "      copying hopper\\benchmark_attn.py -> build\\lib.win-amd64-cpython-312\\hopper\n",
      "      copying hopper\\flash_attn_interface.py -> build\\lib.win-amd64-cpython-312\\hopper\n",
      "      copying hopper\\setup.py -> build\\lib.win-amd64-cpython-312\\hopper\n",
      "      copying hopper\\test_flash_attn.py -> build\\lib.win-amd64-cpython-312\\hopper\n",
      "      copying hopper\\__init__.py -> build\\lib.win-amd64-cpython-312\\hopper\n",
      "      creating build\\lib.win-amd64-cpython-312\\flash_attn\\layers\n",
      "      copying flash_attn\\layers\\patch_embed.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\layers\n",
      "      copying flash_attn\\layers\\rotary.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\layers\n",
      "      copying flash_attn\\layers\\__init__.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\layers\n",
      "      creating build\\lib.win-amd64-cpython-312\\flash_attn\\losses\n",
      "      copying flash_attn\\losses\\cross_entropy.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\losses\n",
      "      copying flash_attn\\losses\\__init__.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\losses\n",
      "      creating build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\baichuan.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\bert.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\bigcode.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\btlm.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\falcon.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\gpt.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\gptj.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\gpt_neox.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\llama.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\opt.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\vit.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      copying flash_attn\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\models\n",
      "      creating build\\lib.win-amd64-cpython-312\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\block.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\embedding.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\mha.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\mlp.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\modules\n",
      "      copying flash_attn\\modules\\__init__.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\modules\n",
      "      creating build\\lib.win-amd64-cpython-312\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\activations.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\fused_dense.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\layer_norm.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\rms_norm.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\n",
      "      copying flash_attn\\ops\\__init__.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\n",
      "      creating build\\lib.win-amd64-cpython-312\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\benchmark.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\distributed.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\generation.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\pretrained.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\utils\n",
      "      copying flash_attn\\utils\\__init__.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\utils\n",
      "      creating build\\lib.win-amd64-cpython-312\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\cross_entropy.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\k_activations.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\layer_norm.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\linear.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\mlp.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\rotary.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\\triton\n",
      "      copying flash_attn\\ops\\triton\\__init__.py -> build\\lib.win-amd64-cpython-312\\flash_attn\\ops\\triton\n",
      "      running build_ext\n",
      "      C:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:380: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "      building 'flash_attn_2_cuda' extension\n",
      "      creating build\\temp.win-amd64-cpython-312\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\csrc\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\csrc\\flash_attn\n",
      "      creating build\\temp.win-amd64-cpython-312\\Release\\csrc\\flash_attn\\src\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Temp\\pip-install-2su4qfpy\\flash-attn_565dbab6bc4e4de5a26b8f8422d9a612\\csrc\\flash_attn\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Temp\\pip-install-2su4qfpy\\flash-attn_565dbab6bc4e4de5a26b8f8422d9a612\\csrc\\flash_attn\\src\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Temp\\pip-install-2su4qfpy\\flash-attn_565dbab6bc4e4de5a26b8f8422d9a612\\csrc\\cutlass\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\include\\TH\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\include\\THC\" \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" /EHsc /Tpcsrc/flash_attn/flash_api.cpp /Fobuild\\temp.win-amd64-cpython-312\\Release\\csrc/flash_attn/flash_api.obj /MD /wd4819 /wd4251 /wd4244 /wd4267 /wd4275 /wd4018 /wd4190 /wd4624 /wd4067 /wd4068 /EHsc -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 /std:c++17\n",
      "      cl : Command line warning D9002 : ignoring unknown option '-O3'\n",
      "      cl : Command line warning D9002 : ignoring unknown option '-std=c++17'\n",
      "      flash_api.cpp\n",
      "      \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\\nvcc\" -c csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.cu -o build\\temp.win-amd64-cpython-312\\Release\\csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.obj \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Temp\\pip-install-2su4qfpy\\flash-attn_565dbab6bc4e4de5a26b8f8422d9a612\\csrc\\flash_attn\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Temp\\pip-install-2su4qfpy\\flash-attn_565dbab6bc4e4de5a26b8f8422d9a612\\csrc\\flash_attn\\src\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Temp\\pip-install-2su4qfpy\\flash-attn_565dbab6bc4e4de5a26b8f8422d9a612\\csrc\\cutlass\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\include\\torch\\csrc\\api\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\include\\TH\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\include\\THC\" \"-IC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\include\" \"-IC:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcompiler /EHsc -Xcompiler /wd4068 -Xcompiler /wd4067 -Xcompiler /wd4624 -Xcompiler /wd4190 -Xcompiler /wd4018 -Xcompiler /wd4275 -Xcompiler /wd4267 -Xcompiler /wd4244 -Xcompiler /wd4251 -Xcompiler /wd4819 -Xcompiler /MD -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17 --use-local-env\n",
      "      flash_bwd_hdim128_bf16_causal_sm80.cu\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'\n",
      "      flash_bwd_hdim128_bf16_causal_sm80.cu\n",
      "      C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include\\crt/host_config.h(153): fatal error C1189: #error:  -- unsupported Microsoft Visual Studio version! Only the versions between 2017 and 2022 (inclusive) are supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'\n",
      "      flash_bwd_hdim128_bf16_causal_sm80.cu\n",
      "      C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include\\crt/host_config.h(153): fatal error C1189: #error:  -- unsupported Microsoft Visual Studio version! Only the versions between 2017 and 2022 (inclusive) are supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_OPERATORS__' with '/U__CUDA_NO_HALF_OPERATORS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF_CONVERSIONS__' with '/U__CUDA_NO_HALF_CONVERSIONS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_HALF2_OPERATORS__' with '/U__CUDA_NO_HALF2_OPERATORS__'\n",
      "      cl : Command line warning D9025 : overriding '/D__CUDA_NO_BFLOAT16_CONVERSIONS__' with '/U__CUDA_NO_BFLOAT16_CONVERSIONS__'\n",
      "      flash_bwd_hdim128_bf16_causal_sm80.cu\n",
      "      C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include\\crt/host_config.h(153): fatal error C1189: #error:  -- unsupported Microsoft Visual Studio version! Only the versions between 2017 and 2022 (inclusive) are supported! The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\n",
      "      error: command 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.8\\\\bin\\\\nvcc.exe' failed with exit code 4294967295\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for flash-attn\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (flash-attn)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers==4.45.0 accelerate==0.34.1 sentencepiece==0.2.0 torchvision requests torch Pillow\n",
    "pip install flash-attn --no-build-isolation\n",
    "\n",
    "# # For better performance, you can install grouped-gemm, which may take 3-5 minutes to install\n",
    "# !pip install grouped_gemm==0.1.6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Manjusha Kumari\\.cache\\huggingface\\hub\\models--rhymes-ai--Aria. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "A new version of the following files was downloaded from https://huggingface.co/rhymes-ai/Aria:\n",
      "- moe_lm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/rhymes-ai/Aria:\n",
      "- projector.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/rhymes-ai/Aria:\n",
      "- vision_encoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/rhymes-ai/Aria:\n",
      "- configuration_aria.py\n",
      "- vision_encoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/rhymes-ai/Aria:\n",
      "- modeling_aria.py\n",
      "- moe_lm.py\n",
      "- projector.py\n",
      "- configuration_aria.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`grouped_gemm` is not installed, using sequential GEMM, which is slower.\n",
      "Downloading shards: 100%|██████████| 12/12 [2:25:01<00:00, 725.15s/it] \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoProcessor\n\u001b[0;32m      6\u001b[0m model_id_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrhymes-ai/Aria\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3880\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3874\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   3875\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   3876\u001b[0m )\n\u001b[0;32m   3878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   3879\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 3880\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3882\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   3883\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\rhymes-ai\\Aria\\90fb8b483b0936ce3cdbbeb670202587b06c5c0c\\modeling_aria.py:136\u001b[0m, in \u001b[0;36mAriaForConditionalGeneration.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: AriaConfig):\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_tower \u001b[38;5;241m=\u001b[39m \u001b[43mAriaVisionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_modal_projector \u001b[38;5;241m=\u001b[39m build_mm_projector(config)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtext_config\u001b[38;5;241m.\u001b[39mvocab_size\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\rhymes-ai\\Aria\\90fb8b483b0936ce3cdbbeb670202587b06c5c0c\\vision_encoder.py:88\u001b[0m, in \u001b[0;36mAriaVisionModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: AriaVisionConfig):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model \u001b[38;5;241m=\u001b[39m AriaVisionTransformer(config)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\siglip\\modeling_siglip.py:1145\u001b[0m, in \u001b[0;36mSiglipVisionModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: SiglipVisionConfig):\n\u001b[1;32m-> 1145\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model \u001b[38;5;241m=\u001b[39m SiglipVisionTransformer(config)\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1404\u001b[0m, in \u001b[0;36mPreTrainedModel.__init__\u001b[1;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1399\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter config in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(config)` should be an instance of class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PretrainedConfig`. To create a model from a pretrained model use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1402\u001b[0m     )\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;66;03m# Save config and origin of the pretrained weights if given in model\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_or_path \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mname_or_path\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1572\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[1;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[0;32m   1569\u001b[0m     config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1572\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;66;03m# use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\u001b[39;00m\n\u001b[0;32m   1581\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_sdpa(\n\u001b[0;32m   1582\u001b[0m         config,\n\u001b[0;32m   1583\u001b[0m         hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1584\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:1693\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[1;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[0;32m   1690\u001b[0m install_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1695\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n",
      "\u001b[1;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "model_id_or_path = \"rhymes-ai/Aria\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id_or_path, trust_remote_code=True)\n",
    "\n",
    "image_path = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\"\n",
    "\n",
    "image = Image.open(requests.get(image_path, stream=True).raw)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": None, \"type\": \"image\"},\n",
    "            {\"text\": \"what is the image?\", \"type\": \"text\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\")\n",
    "inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(model.dtype)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,\n",
    "        stop_strings=[\"<|im_end|>\"],\n",
    "        tokenizer=processor.tokenizer,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "    output_ids = output[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    result = processor.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
